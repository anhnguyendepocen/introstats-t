---
title: 'chapter 4 Basic Data Analysis'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background']
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# The Distribution of Data

As you saw in the previous chapter, data is often presented as a collection of observations, or a **distribution**, which represents some variable. Histograms are a good way to visualize the distribution of a variable:    

```{r}
ggplot(aes(x = PropFemaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = ..density..)) +
  xlab('Proportion of females in top-earning quartile') +
  theme_bw()
```

**Summary statistics** can help extract meaning from a distribution of data. The `summary()` function produces a few common ones: 

```{r}
summary(paygap$PropFemaleTopQuartile)
```

\ 

## Measures of central tendency

Measures of central tendency indicate the *central* or *typical* value of a distribution. The most common is the mean, though others are useful in certain situations.   

### Mean {-}

The mean of a set of observations is the sum of all observations divided by the number of observations: 

$$\bar X = \frac 1n \sum_i^n X_i$$

where $X_i$ is an individual observation in the set, and $n$ is the number of observations.  

In R you can use the `mean()` function:

```{r}
mean(paygap$PropFemaleTopQuartile)
```


### Median {-}

The median is the middle value or the 50th percentile. Half the observations are below (and above) the median: 

$$m = \frac 12 \big( X_{(n/2)} + X_{(n/2+1)} \big) \hspace{0.5cm} \text{or} \hspace{0.5cm} X_{(n+1)/2)}$$

In R you can use the `median()` function: 

```{r}
median(paygap$PropFemaleTopQuartile)
```


### Mode {-}

The mode is the most commonly occurring value. Although there is no function in R to formally calculate the mode, you can visualize it as the value corresponding to the peak in a histogram or density plot.  

\ 

## Measures of dispersion 

Measures of dispersion indicate the *spread* or *variation* of a distribution. The most common is the standard deviation. 

### Standard deviation {-}

The standard deviation of a set of observations is the average distance of each observation from the mean:

$$s = \sqrt{\frac{1}{n-1} \sum_i^n (X_i - \bar X)^2}$$

Note the denominator is $n-1$, not $n$. Dividing by $n$ will produce an underestimate if the data in question is a **sample** (which the pay gap data is).  

In R you can use the `sd()` function: 

```{r}
sd(paygap$PropFemaleTopQuartile)
```

The formula for standard deviation is derived from **variance**, which is the average *squared* distance of each observation from the mean:

$$s^2 = \frac{1}{n-1} \sum_i^n (X_i - \bar X)^2$$

### Quartiles {-}

The **lower quartile** is the 25th percentile of the distribution (one quarter of the observations are below it). 

The **upper quartile** is the 75th percentile of the distribution (one quarter of the observations are above it). 

\ 

## Skew

In a perfectly symmetric distribution, the mean, median, and mode are equal. Disparities indicate the distribution is skewed.  

Compare the distributions for females and males in the top-earning quartile:  

```{r, echo=FALSE, fig.width=10, fig.height=5}
plot1 <- ggplot(aes(x = PropFemaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = stat(density))) +
  xlab('Proportion of Females in the Top-Earning Quartile') + 
  geom_vline(xintercept = mean(paygap$PropFemaleTopQuartile), linetype = 'dashed', color = 'red') +
  annotate('text', x = 0.47, y = 2, label = 'Mean', color = 'red') + 
  geom_vline(xintercept = median(paygap$PropFemaleTopQuartile), linetype = 'dashed', color = 'blue') + 
  annotate('text', x = 0.30, y = 2, label = 'Median', color = 'blue') + 
  geom_vline(xintercept = 0.105, linetype = 'dashed', color = 'violet') + 
  annotate('text', x = 0.17, y = 2, label = 'Mode', color = 'violet') + 
  theme_bw()

plot2 <- ggplot(aes(x = PropMaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = stat(density))) +
  xlab('Proportion of Males in the Top-Earning Quartile') + 
  geom_vline(xintercept = mean(paygap$PropMaleTopQuartile), linetype = 'dashed', color = 'red') +
  annotate('text', x = 0.51, y = 2, label = 'Mean', color = 'red') + 
  geom_vline(xintercept = median(paygap$PropMaleTopQuartile), linetype = 'dashed', color = 'blue') + 
  annotate('text', x = 0.71, y = 2, label = 'Median', color = 'blue') + 
  geom_vline(xintercept = 0.90, linetype = 'dashed', color = 'violet') + 
  annotate('text', x = 0.83, y = 2, label = 'Mode', color = 'violet') + 
  theme_bw()

grid.arrange(plot1, plot2, ncol = 2)
```

Naturally the male and female distributions are mirror images of each other (due to the cis-normativity of the study).  

The female distribution has **right-skew** (or positive skew), since the tail drags rightwards (in the positive direction). The male distribution has **left-skew** (negative skew), since the tail drags leftwards (in the negative direction).  

- under right skew, mean $>$ median $>$ mode
- under left skew, mode $>$ median $>$ mean

Formally, the **skewness** of a distribution is defined as:

$$g_1 = \frac{\frac 1n \sum_i^n (X_i - \bar X)^3}{s^3}$$

In R you can use the `skewness()` function (from the package `moments`):

```{r}
skewness(paygap$PropFemaleTopQuartile)
```

The positive value indicates the female distribution is positively skewed. Predictably, the skewness of the male distribution is:

```{r}
skewness(paygap$PropMaleTopQuartile)
```


\ 

## Outliers 

Outliers are data points that differ substantially from other observations. The presence of outliers in a sample indicates either:

1. there is measurement error in the experiment
2. the population has a skewed/heavy-tailed distribution  

Outliers due to measurement error should be discarded as they unduly skew results. Below is an example from a classic dataset measuring the speed of light in 1879:  

```{r, echo=FALSE}
library(datasets)

ggplot(aes(x = as.factor(Expt), y = Speed + 299000), data = morley) + 
  geom_boxplot() +
  ggtitle('Michelson-Morley Experimental Data 1879') + 
  xlab('experiment no.') + ylab('speed of light (km/s)') +
  theme_bw()
```

Boxplots are a good way to visualize outliers in a distribution of data. In this example, experiments 1 and 3 have outliers.  

If outliers are not due to measurement error, they may suggest the distribution is intrinsically **skewed** or **heavy-tailed**. An example is the distribution of females/males in the top-earning quartile. Provided the observations are not erroneous, the skew of these distributions may reflect the inherent asymmetry in the distribution of females/males across income (assuming, of course, that the data is **unbiased**; this is a big assumption that ought to be tested before any conclusions are drawn. See: bias. `insert link`).  

In general, outliers should not be discarded unless they are obviously due to measurement error.  

### Mean vs. median {-}

In the presence of outliers, **the mean is more susceptible to skew than the median**. This is because the mean is weighted by each observation in a sample. The median is not -- it is simply the middle value.  

E.g. in the following sample, nine observations are between 21 and 22, and one outlier is above 80:

```{r, echo=FALSE}
X <- c(round(runif(n = 9, min = 21, max = 22),1), 83.4)
#kable(t(X))
```

The mean of this sample is $\bar X =$ `r toString(round(mean(X), 2))` and the median is $m =$ `r toString(round(median(X), 2))`. Clearly the median is a more representative measure of central tendency than the mean.  

Strong outliers drag the mean in the direction of skew (in this case positive). This is why under positive skew the mean is larger than the median, and vice versa.  

Estimators are said to be **robust** if they can cope with outliers. The median is a robust measure of central tendency; the mean is not. However the mean is generally a more precise statistic.   

### Median absolute deviation (MAD) {-}

The median absolute deviation is a robust measure of dispersion. Mathematically it is defined as the median of the absolute deviations of each observation from the data's median: 

$$\text{MAD} = \text{median}(|X_i - m|)$$

where $m$ is the median of the data.  

The MAD is more resilient to outliers than the standard deviation is, since the former uses absolute deviations, while the latter uses *squared* deviations.  

In R, you can use the `mad()` function (from the package `stats`): 

```{r}
mad(paygap$PropFemaleTopQuartile)
```



\ 

--- 

# Intervals

```{r, include=FALSE}
# use 2019 dataset for this section
# because the 2018 data has too many observations
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
```

Observations in a dataset constitute a **sample**. If you conducted another experiment, you would get a different sample, and you would get different values for summary statistics such as the mean and median.  

The sample mean, $\bar X$, is thus an **estimate** -- specifically, a **point estimate** -- for the true mean. 

When estimating a parameter it is often more useful to provide a range of values -- an **interval estimate** -- rather than a point estimate.  

\ 

## Confidence intervals 

A **confidence interval** is a range of values that might contain the true parameter. Every confidence interval is associated with a **confidence level**, which describes the approximate probability the true parameter lies in the interval specified. The theory behind this is explained in part 2. 

Although there is no function in R to formally compute a confidence interval for a parameter, it is simple enough to write one yourself. 

The code below creates a function that takes a vector array of data and computes a confidence interval for the mean. The function takes two inputs: `data`: a vector array of data, and `conflevel`: the desired confidence level.  

```{r}
confidence_interval <- function(data, conflevel) {
  xbar <- mean(data)          # sample mean 
  n <- length(data)           # sample size 
  SE <- sd(data) / sqrt(n)    # standard error
  alpha <- 1 - conflevel      # alpha
  
  lb <- xbar + qt(alpha/2, df = n-1) * SE    # lower bound
  ub <- xbar + qt(1-alpha/2, df = n-1) * SE  # upper bound
  
  cat(paste(c('sample mean =', round(xbar,3), '\n', 
              conflevel*100, '% confidence interval:', '\n', 
              'lower bound =', round(lb,3), '\n', 
              'upper bound =', round(ub,3))))
}
```

You can use this function to compute the a 95\% confidence interval for the mean female bonus percent in the data:

```{r}
confidence_interval(data = paygap$FemaleBonusPercent, conflevel = 0.95)
```



\ 

--- 

# Simple Hypothesis Tests

A statistical test compares a sample of data to a preexisting model of what the data looks like. The test is used to either validate or reject the model.  

There are two hypotheses in a test: 

- **the null hypothesis**, $H_0$, a proposed/preexisting model for the data 
- **the alternative hypothesis**, $H_1$, that the proposed model is not true 

The test is deemed **statistically significant** if the comparison yields a result that is very unlikely under the null hypothesis (i.e. if there is strong evidence to support the alternative hypothesis). This usually leads to **rejecting** the null hypothesis.  

Some simple tests are introduced below. For more theory on hypothesis testing, go here. `insert link`

\ 

## One-sample t-test for a mean 

A one-sample t-test compares the mean of a sample of data to some hypothesized value. The null and alternative hypotheses are:

- $H_0$: the mean is equal to some specified value $k$
- $H_1$: the mean is not equal to $k$

E.g. in the pay gap data, the variable `DiffMeanHourlyPercent` records the percentage difference in mean hourly wages between women and men for each firm in the sample. You could conduct a one-sample t-test on this variable, where the null hypothesis is that the mean of this variable is zero, as perhaps it ought to be:

$$
\begin{aligned}
  H_0: \texttt{mean(DiffMeanHourlyPercent)} = 0 \\ 
  H_1: \texttt{mean(DiffMeanHourlyPercent)} \neq 0
\end{aligned}
$$

In R, you can use the `t.test()` function. The first argument should be the sample you are testing, and the second is the proposed value for the mean under the null hypothesis: 

```{r}
t.test(paygap$DiffMeanHourlyPercent, mu = 0)
```

The observed value of this test is printed at the bottom: $\bar X =$ 12.36. This tells you that the average percentage difference between female and male hourly wages is 12.36\%, according to this sample.  

Now it is up to you whether to reject the null hypothesis or not, on the basis of this evidence. You can use the $p$-value of the test to help you make a decision.  

**The $p$-value of a test is the probability of getting a value at least as extreme as the observed value under the null hypothesis.** The $p$-value will always be between 0 and 1. A small $p$-value means the observed value is unlikely to occur if the null hypothesis were true. A large $p$-value means the observed value is likely to occur if the null hypothesis were true.  

The $p$-value of this test is printed in the third line: `p-value < 2.2e-16`. This is very small. It means there is a near zero probability of observing a 12.36\% difference in wages if in reality the true difference in wages is zero.  

In other words, this test gives evidence to reject the null hypothesis. You may thus choose to conclude that the average difference in hourly wages between women and men is not zero, based on the evidence in this sample.  

\ 

## Interpreting p-values 

It is conventional to reject the null hypothesis if the $p$-value of a test is smaller than 0.05. However this is an arbitrary (and disputable) cutoff point, and you should use your own intuition to determine whether rejecting the null is a sensible choice, given the context.  

You can, for instance, use confidence intervals to determine whether rejecting the null is sensible. Note how the output of the $t$-test also gives you a 95\% confidence interval for the true mean, based on the sample data. In the example above, it suggests the true mean difference in wages is somewhere between 9.8\% and 14.9\%. Since this range is still substantially above zero, it supports your decision to reject the null.  

\ 

## Two-sample t-test for a difference in means

A two-sample $t$-test compares the means of two samples of data. The null and alternative hypotheses are as follows:

- $H_0$: the difference in the means of both samples is equal to zero (i.e. the samples have the same mean)
- $H_1$: the difference in the means of both samples is not equal to zero  (the samples have different means)

E.g. in the pay gap data, the variables `FemaleBonusPercent` and `MaleBonusPercent` record the average percentage bonus at each firm, for females and males respectively. You could construct a two-sample $t$-test between these variables, where the null hypothesis is that the samples have the same mean:

$$
\begin{aligned}
  H_0: \texttt{|mean(FemaleBonusPercent) - mean(MaleBonusPercent)|} = 0 \\ 
  H_1: \texttt{|mean(FemaleBonusPercent) - mean(MaleBonusPercent)|} \neq 0
\end{aligned}
$$

In R you can use the `t.test()` function, entering both samples as arguments: 

```{r}
t.test(paygap$FemaleBonusPercent, paygap$MaleBonusPercent)
```

The observed value of this is at the bottom: the average bonus percent is 25.5\% for females and 26.0\% for males, making the absolute difference between the two 0.5\%. Though nonzero, this difference is small.  

The $p$-value of this test is 0.889, i.e. there is an 88.9\% of seeing an observed difference of 0.5\% under the null hypothesis. Since this $p$-value is substantially higher than 0.05, there is not sufficient evidence to reject the null. Moreover, the 95\% confidence interval *contains* the null hypothesis (that the difference is zero).  

You should thus assume the null is true, and conclude that there is no evidence in this sample to suggest the average bonus percent is different for females and males. 

\ 

## Chi-squared test for independence 

...


\ 

---

# Covariation 

Histograms and summary statistics help understand the variation of data *within* a variable. **Covariation** describes the variation *between* variables -- specifically, the tendency of variables to vary together in a related way. 

\ 

## Visualizing covariation

Scatterplots can help visualize the covariation of two continuous variables. 

The dataset below contains information on the temperature and urban characteristics of various locations in New York City during a recent heatwave. A preview of the data is shown:

```{r}
nycheat <- read.csv('./data/nyc-heatwave.csv')

# kable(nycheat[1:5, ], caption = 'NYC Heatwave Data') %>%
#   kable_styling(latex_options = c('hold_position','scale_down'))
```

\ 

NDVI (Normalized Difference Vegetation Index) is a measure of the concentration of vegetation in the recorded area. Albedo is the tendency of buildings to reflect solar radiation in the area, e.g. a value of 5 means buildings reflect on average 5\% of solar radiation (and absorb 95\%).  

Below is a scatterplot of temperature on NDVI: 

```{r}
ggplot(aes(x = ndvi, y = temperature), data = nycheat) +
  geom_point(size=1) +
  xlab('NDVI (vegetation index)') + ylab('temperature (farenheit)') +
  theme_bw()
```

There appears to be a negative relationship between temperature and NDVI, i.e. lower temperatures seem to be recorded in areas with a higher concentration of vegetation, and vice versa.  

\ 

## Categorical variables

You can visualize the covariation between a continuous variable and a categorical variable with a boxplot. E.g. the covariation between temperature (continuous) and area (categorical): 

```{r, include=FALSE}
nycheat <- nycheat %>%
  mutate(area = factor(area, levels = c('mid-manhattan west','lower manhattan east',
                                        'fordham bronx','maspeth queens',
                                        'crown heights brooklyn','ocean parkway brooklyn')))
```

```{r, fig.width=7, fig.height=4}
ggplot(aes(x = area, y = temperature, color = area), data = nycheat) +
  geom_boxplot() +
  scale_x_discrete(labels = c('mid- \n manhattan \n west', 'lower \n manhattan \n east', 'fordham \n bronx', 'maspeth \n queens', 'crown \n heights \n brooklyn', 'ocean \n parkway \n brooklyn')) + 
  theme_bw()
```

While the two Brooklyn locations appear to have higher temperatures, the other locations have relatively consistent means.  

Similarly in the pay gap data, the covariation between employer size (categorical) and the percentage difference in mean hourly wages between females and males (continuous):

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2018.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
```

```{r, fig.width=7, fig.height=4}
ggplot(aes(x = EmployerSize, y = DiffMeanHourlyPercent, color = EmployerSize), data = paygap) +
  xlab('employer size') + ylab('percent difference in mean hourly wages') + 
  geom_boxplot() +
  theme_bw()
```

The mean difference in wages appears to be relatively consistent across all firm sizes, but smaller firms seem to have higher variation.    

\ 

## Correlation 

Correlation is a measure of the *strength* of the relationship between two variables. 

The stronger the correlation, the stronger the (apparent) relationship between the two variables. Strong correlation is depicted by tight clustering of data points, close to the diagonal line $y = x$ (or $y=-x$ for negative correlation). Weak correlation is depicted by loose clustering of data points, scattered away from the diagonal line.  

```{r, echo=FALSE}
knitr::include_graphics('./pics/m1c4_pic1.png')
```


\ 

## The Pearson correlation coefficient, $r$

The most common measure of correlation is the Pearson correlation coefficient, $r$, which takes a value between -1 and 1. A coefficient of $r=1$ or $r=-1$ implies perfect correlation, i.e. every data point is on the diagonal line. $r=0$ implies no correlation.  

```{r, echo=FALSE}
knitr::include_graphics('./pics/m1c4_pic3.png')
```

The Pearson correlation coefficient is only responsive to linear relationships. In the above figure, the bottom row shows variables which are clearly related to each other (indicated by the pattern) but since the relationship is nonlinear, the Pearson correlation coefficient is zero.  

In R you can use the `cor()` function to compute the Pearson correlation coefficient between two variables. E.g. in the NYC heatwave data, the correlation between temperature and vegetation index is:

```{r}
cor(nycheat$temperature, nycheat$ndvi)
```

A correlation coefficient of $r=-0.57$ suggests a moderate negative relationship between the two variables, as the scatterplot depicts.    

\ 

## Correlation does not imply causation 

Simply because two variables are correlated does not imply they are *causally* related. Demonstrating correlation is easy -- but unless there is a clear causal mechanism between the two variables, it is much harder to prove that one actually causes the other. It is on you, the experimenter, to demonstrate a causal mechanism in your analysis.  

This webpage lists examples of **spurious correlations** -- variables that are technically correlated but which are obviously unrelated.  

https://www.tylervigen.com/spurious-correlations 





