---
title: 'chapter 4 Basic Data Analysis'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background']
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2018.csv')

paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))

nycheat <- read.csv('./data/nyc-heatwave.csv')

nycbnb <- read.csv('./data/nyc-airbnb.csv', header = TRUE) %>%
  mutate(price = as.numeric(price), 
         cleaningfee = as.numeric(cleaningfee), 
         availability365 = as.numeric(availability365), 
         sqft = as.numeric(sqft))
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# Of Populations and Samples

Every experiment or study is concerned with answering some question. Some (possibly relevant) examples--

- what is the true extent of the gender pay gap?
- why is New York City insufferably hot in summer?

Conducting a study involves choosing relevant variables to answer the question, collecting empirical data on these variables, and using some method of analysis to answer the question.  

Every study has a **population**--this is the relevant set of events or observations required to fully answer the question. E.g. if trying to determine the gender pay gap, the population might be the salary of every single person.  

Before conducting a study, there is often a **hypothesis** or preexisting notion about what the population looks like. The data collected during the study is often used to validate or reject the hypothesis.  

A **sample** is a subset of the events/observations in a population. Most studies are conducted with samples, since it is often impossible to acquire data on a whole population. 

Below are two examples of sample data collected in order to investigate certain research questions.  

\ 

## Concerning the gender pay gap

The following dataset has variables characterising the gender pay gap at over 10,000 UK firms in 2018.^[The data is freely available at https://gender-pay-gap.service.gov.uk/. Excuse the horribly cumbersome variable names.]  

Below is a preview of the dataset and some of the variables contained therein:

```{r, echo=FALSE}
kable(paygap[c(323,343,521,1387,1389,1495,3313,3889,4884,6214,7294,7299,10029), c(1:3,7:10)])
```

\ 

## Concerning the summer climate of New York

The following dataset has variables characterising the temperature and urban features of over 1000 locations in the city during a heatwave.^[Download the data <a href="base64 data" download="./data/nyc-heatwave.csv">here</a>. This data was simulated from the findings of this <a href="base64 data" download="./downloads/nyc-heat-island-mitigation.pdf">study</a>, investigating the extreme heating effect in NYC. Additional sources: <a href="https://www.ncdc.noaa.gov/cdo-web/search">NOAA</a>, <a href="https://landsat.visibleearth.nasa.gov/view.php?id=6800">Landsat</a>, <a href="https://data.cityofnewyork.us/Environment/Landcover-Raster-Data-2010-3ft-Resolution/9auy-76zt">NYC OpenData</a>.] Each recorded location was in one of six geographical areas. Below is a preview of the dataset:

```{r, echo=FALSE}
kable(nycheat[c(1:3, 196:198, 407:409, 524:526, 807:809, 898:900), ])
```



\ 

---

# The Distribution of Data

When presented with a sample of data, it is useful to first understand its **distribution**. As demonstrated in the previous chapter, histograms are a good way to visualize the **distribution** of observations within a variable.  

From the pay gap data, below is a histogram of the variable `PropFemaleTopQuartile` -- the proportion of females in the top-earning quartile. 

```{r}
ggplot(aes(x = PropFemaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = ..density..)) +
  xlab('proportion of females in top-earning quartile') +
  theme_bw()
```

**Summary statistics** can help extract meaning from a distribution of data. The `summary()` function produces a few common ones: 

```{r}
summary(paygap$PropFemaleTopQuartile)
```

Summary statistics are point values that attempt to summarize some characteristic of a distribution, e.g. central tendency, spread, skew, etc. 

\ 

## Measures of central tendency

Measures of central tendency indicate the central or typical value of a distribution. The most common is the mean, though others are useful in certain situations.   

\ 

**Mean**

The mean of a set of observations is the sum of all observations divided by the number of observations. For sample data this statistic is known as the **sample mean**:

$$\bar X = \frac 1n \sum_i^n X_i$$

where $X_i$ is an individual observation in the set, and $n$ is the number of observations.  

In R you can use `mean()`:

```{r}
mean(paygap$PropFemaleTopQuartile)
```

\ 

**Median**

The median is the middle value or the 50th percentile of a distribution. Half the observations are below (and above) the median: 

$$m = \frac 12 \big( X_{(n/2)} + X_{(n/2+1)} \big) \hspace{0.5cm} \text{or} \hspace{0.5cm} X_{(n+1)/2)}$$

In R you can use `median()`: 

```{r}
median(paygap$PropFemaleTopQuartile)
```

\ 

**Mode**

```{r, echo=FALSE, fig.margin = TRUE, fig.cap = "The mode can be visualized as follows."}
ggplot(aes(x = PropFemaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = ..density..)) +
  xlab('proportion of females in top-earning quartile') +
  geom_vline(xintercept = 0.102, linetype = 'dashed', color = 'violet') + 
  annotate('text', x = 0.17, y = 2, label = 'Mode', color = 'violet', size=5) + 
  theme_bw()
```

The mode is the most commonly occurring value in a distribution. Although there is no function in R to formally calculate the mode, you can visualize it as the value corresponding to the peak in a histogram or density plot. 




\ 

## Measures of dispersion 

Measures of dispersion indicate the spread or variation of a distribution. The most common is the standard deviation. 

\ 

**Standard Deviation**

The standard deviation of a set of observations is the average distance of each observation from the mean. For sample data this statistic is known as the **sample standard deviation**:^[Note the denominator in the formula is $n-1$, not $n$. This is known as Bessel's correction, and is used when calculating the standard deviation of a **sample** of data. More on this later.]

$$s = \sqrt{\frac{1}{n-1} \sum_i^n (X_i - \bar X)^2}$$

In R you can use `sd()`:^[Note the `sd()` function in R computes the sample standard deviation, i.e. it uses the $n-1$ denominator.] 

```{r}
sd(paygap$PropFemaleTopQuartile)
```

The formula for standard deviation is derived from **variance**--the average squared distance of each observation from the mean:

$$s^2 = \frac{1}{n-1} \sum_i^n (X_i - \bar X)^2$$

\ 

**Quartiles**

The **lower quartile** is the 25th percentile of the distribution (one quarter of the observations are below it). 

The **upper quartile** is the 75th percentile of the distribution (one quarter of the observations are above it). 

\ 

## Skew

In a perfectly symmetric distribution, the mean, median, and mode are equal. Disparities indicate the distribution is skewed.  

Compare the distributions for females vs males in the top-earning quartile:  

```{r, echo=FALSE, fig.width=10, fig.height=5}
plot1 <- ggplot(aes(x = PropFemaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = stat(density))) +
  xlab('proportion of females in the top-earning quartile') + 
  geom_vline(xintercept = mean(paygap$PropFemaleTopQuartile), linetype = 'dashed', color = 'red') +
  annotate('text', x = 0.47, y = 2, label = 'Mean', color = 'red') + 
  geom_vline(xintercept = median(paygap$PropFemaleTopQuartile), linetype = 'dashed', color = 'blue') + 
  annotate('text', x = 0.30, y = 2, label = 'Median', color = 'blue') + 
  geom_vline(xintercept = 0.105, linetype = 'dashed', color = 'violet') + 
  annotate('text', x = 0.17, y = 2, label = 'Mode', color = 'violet') + 
  theme_bw()

plot2 <- ggplot(aes(x = PropMaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = stat(density))) +
  xlab('proportion of males in the top-earning quartile') + 
  geom_vline(xintercept = mean(paygap$PropMaleTopQuartile), linetype = 'dashed', color = 'red') +
  annotate('text', x = 0.51, y = 2, label = 'Mean', color = 'red') + 
  geom_vline(xintercept = median(paygap$PropMaleTopQuartile), linetype = 'dashed', color = 'blue') + 
  annotate('text', x = 0.71, y = 2, label = 'Median', color = 'blue') + 
  geom_vline(xintercept = 0.90, linetype = 'dashed', color = 'violet') + 
  annotate('text', x = 0.83, y = 2, label = 'Mode', color = 'violet') + 
  theme_bw()

grid.arrange(plot1, plot2, ncol = 2)
```

Naturally^[Due to the inherent cis-normativity of the study.] these distributions are mirror images of each other. 

The distribution for females has **right-skew** (or positive skew), since the tail drags rightwards (in the positive direction). The distribution for males has **left-skew** (negative skew), since the tail drags leftwards (in the negative direction).  

- under right skew, mean $>$ median $>$ mode
- under left skew, mode $>$ median $>$ mean

Formally, the **skewness** of a sample of data is defined as: 

$$g_1 = \frac{\frac 1n \sum_i^n (X_i - \bar X)^3}{s^3}$$

In R you can use the `skewness()` function:^[From the `moments` package.]

```{r}
skewness(paygap$PropFemaleTopQuartile)
```

The positive value indicates the distribution for females is positively skewed. Predictably, the skewness of the distribution for males is:

```{r}
skewness(paygap$PropMaleTopQuartile)
```


\ 

## Outliers 

Outliers are data points that differ substantially from other observations. The presence of outliers in a sample indicates either:

1. there is measurement error in the experiment
2. the population has a skewed/heavy-tailed distribution  

Outliers due to measurement error should be discarded as they unduly skew results. Below is an example from a classic dataset measuring the speed of light in 1879:  

```{r, echo=FALSE}
library(datasets)

ggplot(aes(x = as.factor(Expt), y = Speed + 299000), data = morley) + 
  geom_boxplot() +
  ggtitle('Michelson-Morley Experimental Data 1879') + 
  xlab('experiment no.') + ylab('speed of light (km/s)') +
  theme_bw()
```

Box and whisker plot are a good way to visualize outliers in a distribution of data. In this example, experiments 1 and 3 have outliers.  

If outliers are not due to measurement error, they may suggest the distribution is intrinsically **skewed** or **heavy-tailed**. 

In general, outliers should not be discarded unless they are obviously due to measurement error.  

\ 

**Mean vs. Median**

In the presence of outliers, the mean is more susceptible to skew than the median. This is because the mean is weighted by each observation in a sample. The median is not--it is simply the middle value.  

E.g. in the following sample, nine observations are between 21 and 22, and one outlier is above 80:

```{r, echo=FALSE}
X <- c(round(runif(n = 9, min = 21, max = 22),1), 83.4)
kable(t(X))
```

The mean of this sample is $\bar X =$ `r toString(round(mean(X), 2))` and the median is $m =$ `r toString(round(median(X), 2))`. In this case the median is clearly a more representative measure of central tendency than the mean.  

Strong outliers drag the mean in the direction of skew (in this case positive). This is why under positive skew the mean is larger than the median, and vice versa.  

Estimators are said to be **robust** if they can cope with outliers. The median is a robust measure of central tendency; the mean is not. However the mean is generally a more precise statistic.   

\ 

**Median Absolute Deviation (MAD)**

The median absolute deviation is a robust measure of dispersion. Mathematically it is defined as the median of the absolute deviations of each observation from the data's median: 

$$\text{MAD} = \text{median}(|X_i - m|)$$

where $m$ is the median of the data.  

The MAD is more resilient to outliers than the standard deviation is, since the former uses absolute deviations, while the latter uses squared deviations.  

In R, you can use `mad()`:^[From the `stats` package.] 

```{r}
mad(paygap$PropFemaleTopQuartile)
```



\ 

--- 

# Point vs Interval Estimators 

```{r, include=FALSE}

```

Summary statistics derived from samples of data--such as the sample mean and sample standard deviation--are known as **estimators**, since they use samples to estimate the true values of population parameters.^[E.g. the *true* mean difference in hourly wages between women and men is unknown; but the sample of pay gap data can be used to estimate it.] 

The summary statistics introduced above are known as **point estimators** since they estimate a true parameter using a single value. 

There also exist **interval estimators**, which estimate a true parameter using a *range* of values. Often these are more useful than single-valued estimates, as they provide reasonable margins of error when drawing inferences from data.

A **confidence interval** is one example of an interval estimator.  



\ 




\ 

---

# Relationships Between Variables 

Histograms and summary statistics help understand the distribution of data within a variable. Scatterplots can help visualize the relationships *between* variables. 

From the heatwave data, below is a scatterplot of temperature on vegetation: 

```{r}
ggplot(aes(x = vegetation, y = temperature), data = nycheat) +
  geom_point(size=1) +
  xlab('vegetation (NDVI)') + ylab('temperature (farenheit)') +
  theme_bw()
```

There appears to be a negative relationship between temperature and vegetation, i.e. lower temperatures seem to be recorded in areas with a higher concentration of vegetation, and vice versa.  

\ 

## Correlation and dependency



   

\ 

## Strong vs weak correlation 

The stronger the correlation, the stronger the (apparent) relationship between the two variables. Strong correlation is depicted by tight clustering of data points, close to the diagonal line $y = x$ (or $y=-x$ for negative correlation). Weak correlation is depicted by loose clustering of data points, scattered away from the diagonal line.  

```{r fig-fullwidth, fig.width = 10, fig.height = 2, echo=FALSE}
knitr::include_graphics('./pics/m1c4_pic1.png')
```


\ 

## The Pearson correlation coefficient, $r$

The most common measure of correlation is the Pearson correlation coefficient, $r$, which takes a value between -1 and 1. A coefficient of $r=1$ or $r=-1$ implies perfect correlation, i.e. every data point is on the diagonal line. $r=0$ implies no correlation.  

```{r, echo=FALSE}
knitr::include_graphics('./pics/m1c4_pic3.png')
```

The Pearson correlation coefficient is only responsive to linear relationships. In the above figure, the bottom row shows variables which are clearly related to each other (indicated by the pattern) but since the relationship is nonlinear, the Pearson correlation coefficient is zero.  

In R you can use `cor()` to compute the Pearson correlation coefficient between two variables. E.g. the Pearson correlation coefficient between temperature and vegetation is:

```{r}
cor(nycheat$temperature, nycheat$vegetation)
```

```{r, fig.margin=TRUE, fig.cap="The relationship between temperature and vegetation. $r=-0.57$"}
ggplot(aes(x = vegetation, y = temperature), data = nycheat) +
  geom_point(size=1) +
  xlab('NDVI (vegetation index)') + ylab('temperature (farenheit)') +
  theme_bw()
```

A correlation coefficient of $r=-0.57$ suggests a moderate negative relationship between the two variables, as the scatterplot suggests.     

\ 

## Correlation does not imply causation 

Simply because two variables are correlated does not imply they are *causally* related. Demonstrating correlation is easy--but unless there is a clear causal mechanism between the two variables, it is much harder to prove that one actually causes the other. It is on you, the experimenter, to demonstrate a causal mechanism in your analysis.  

This webpage lists examples of **spurious correlations**--variables with demonstrable correlation but lacking any causal link. 

https://www.tylervigen.com/spurious-correlations 



\ 

---

\ 






